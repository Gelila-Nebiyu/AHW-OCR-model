import cv2
import matplotlib.pyplot as plt
import numpy as np
from ultralytics import YOLO

def YOLO_Interface(image_path):
    # 1. Load your trained YOLO model
    model = YOLO('/kaggle/input/placeholder/other/default/1/best (1).pt') # Replace with your model path
    
    # 3. Load the image using OpenCV
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error: Could not load image from {image_path}")
        exit()
    
    # Convert BGR to RGB for consistent display with matplotlib later
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    display_image = image_rgb.copy() # Create a copy to draw on
    
    # 4. Perform prediction (no drawing with .plot() here, just get raw results)
    # We set save=False and show=False because we'll handle drawing manually
    results = model.predict(source=image_path, conf=0.5, iou=0.9, save=False, show=False)
    
    # 5. Process results and draw boxes manually using OpenCV
    for r in results:
        if r.boxes: # Check if any bounding boxes were detected
            for box in r.boxes:
                # Get coordinates (xyxy format: xmin, ymin, xmax, ymax)
                # .cpu().numpy() converts the tensor to a NumPy array on the CPU
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
    
                # Get class ID and confidence
                class_id = int(box.cls[0].cpu().numpy())
                confidence = float(box.conf[0].cpu().numpy())
                class_name = model.names[class_id]
    
                
                color = (0, 255, 0) 
                if model.task == 'segment': # Example: different color for segmentation if applicable
                    color = (255, 0, 255) # Magenta
    
                cv2.rectangle(display_image, (x1, y1), (x2, y2), color, 2) 
    
                # Create the label text
                label = f"{class_name} {confidence:.2f}"
    
                # Calculate text size and position to avoid overlap
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.7
                font_thickness = 2
                text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]
    
                # Position text above the box, or inside if space is limited
                text_x = x1
                text_y = y1 - 10 # 10 pixels above the top of the box
    
                # Ensure text is not off-image at the top
                if text_y < 0:
                    text_y = y1 + text_size[1] + 10 # Place below if not enough space above
    
                # Draw background rectangle for text for better readability
                text_bg_color = (0, 0, 0) # Black background
                #cv2.rectangle(display_image, (text_x, text_y - text_size[1] - 5), # top-left
                 #             (text_x + text_size[0] + 5, text_y + 5), # bottom-right
                  #            text_bg_color, -1) # -1 fills the rectangle
    
                #cv2.putText(display_image, label, (text_x + 2, text_y), font,
                 #           font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA) # White text
    
    # 6. Display the annotated image using Matplotlib
    plt.figure(figsize=(12, 10)) # Adjust figure size for better viewing
    plt.imshow(display_image)
    plt.axis('off') # Hide axes
    plt.title('Model Prediction with Custom Labels')
    plt.show()

YOLO_Interface()

import os
import cv2
from ultralytics import YOLO

# Load YOLO model once
model = YOLO('/kaggle/input/placeholder/other/default/1/best (1).pt')

def YOLO_cropper(image_path, output_folder="/kaggle/working/cropped_words", row_tolerance=20):
    """
    Detect words in an image using YOLO, crop them, and save in left-to-right, top-to-bottom order.
    
    Args:
        image_path (str): Path to the input image.
        output_folder (str): Folder where crops will be saved.
        row_tolerance (int): Pixel tolerance for grouping bounding boxes into the same row.
    
    Returns:
        list: A list of file paths to the saved cropped images in reading order.
    """
    os.makedirs(output_folder, exist_ok=True)

    # Run YOLO
    results = model(image_path)

    # Extract bounding boxes
    boxes = results[0].boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]

    # Collect box info
    crops_info = []
    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box[:4])
        crops_info.append((i, x1, y1, x2, y2))

    # Sort: first by row (y1), then by column (x1)
    crops_info.sort(key=lambda b: (b[2] // row_tolerance, b[1]))

    # Load original image
    image = cv2.imread(image_path)

    saved_paths = []
    for idx, (i, x1, y1, x2, y2) in enumerate(crops_info, start=1):
        crop = image[y1:y2, x1:x2]
        save_path = os.path.join(output_folder, f"word_{idx}.jpg")
        cv2.imwrite(save_path, crop)
        saved_paths.append(save_path)

    print(f"‚úÖ Saved {len(saved_paths)} crops in reading order to '{output_folder}'")
    return saved_paths
x = YOLO_cropper('/kaggle/input/dataseet/data/images/test/aahw_0007.jpg')
for i in x:
    print(i)

import torch
import torch.nn as nn

# Same CRNN definition as in training
class CRNN(nn.Module):
    def __init__(self, imgH, nc, nclass, nh):
        super(CRNN, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(nc,64,3,1,1), nn.ReLU(True), nn.MaxPool2d(2,2),
            nn.Conv2d(64,128,3,1,1), nn.ReLU(True), nn.MaxPool2d(2,2),
            nn.Conv2d(128,256,3,1,1), nn.ReLU(True),
            nn.Conv2d(256,256,3,1,1), nn.ReLU(True), nn.MaxPool2d((2,2),(2,1),(0,1)),
            nn.Conv2d(256,512,3,1,1), nn.BatchNorm2d(512), nn.ReLU(True),
            nn.Conv2d(512,512,3,1,1), nn.BatchNorm2d(512), nn.ReLU(True), nn.MaxPool2d((2,2),(2,1),(0,1)),
            nn.Conv2d(512,512,2,1,0), nn.ReLU(True)
        )
        self.rnn1 = nn.LSTM(512, nh, bidirectional=True, dropout=0.3)
        self.rnn2 = nn.LSTM(nh*2, nh, bidirectional=True, dropout=0.3)
        self.fc = nn.Linear(nh*2, nclass)

    def forward(self, x):
        conv = self.cnn(x)
        b, c, h, w = conv.size()
        conv = conv.squeeze(2).permute(2, 0, 1)
        recurrent, _ = self.rnn1(conv)
        recurrent, _ = self.rnn2(recurrent)
        output = self.fc(recurrent)
        return output.permute(1, 0, 2)

  # Device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Recreate model with same hyperparameters
model = CRNN(imgH=32, nc=1, nclass= 303, nh=256).to(DEVICE)

# Load weights
checkpoint = torch.load("/kaggle/input/crnn-amharic-hand-writing-detector/keras/default/1/best_crnn (1).pth", map_location=DEVICE)
model.load_state_dict(checkpoint)

model.eval()
print("Model loaded and ready!")

import cv2
import numpy as np

def preprocess_image(img_path):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    h, w = img.shape
    new_w = max(int(32 * (w / h)), 32)
    img = cv2.resize(img, (new_w, 32))
    img = (img / 255.0).astype(np.float32)
    img = (img - 0.5) / 0.5  # normalize
    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # [B,C,H,W]
    return img.to(DEVICE)

BLANK = 302
def ctc_greedy_decoder(output, idx_to_char, blank=BLANK):
    # output: [B, T, nclass]
    preds = output.softmax(2).argmax(2)  # [B, T]
    preds = preds[0].cpu().numpy().tolist()

    decoded = []
    prev = -1
    for p in preds:
        if p != prev and p != blank:  # collapse repeats, ignore blanks
            decoded.append(idx_to_char.get(p, "ÔøΩ"))
        prev = p
    return "".join(decoded)
  amharic_mapping = {
    # Basic consonants + vowels
    0: '·àÄ', 1: '·àÅ', 2: '·àÇ', 3: '·àÉ', 4: '·àÑ', 5: '·àÖ', 6: '·àÜ',
    7: '·àà', 8: '·àâ', 9: '·àä', 10: '·àã', 11: '·àå', 12: '·àç', 13: '·àé', 14: '·àè',
    15: '·àê', 16: '·àë', 17: '·àí', 18: '·àì', 19: '·àî', 20: '·àï', 21: '·àñ', 22: '·àó',
    23: '·àò', 24: '·àô', 25: '·àö', 26: '·àõ', 27: '·àú', 28: '·àù', 29: '·àû', 30: '·àü',
    31: '·à†', 32: '·à°', 33: '·à¢', 34: '·à£', 35: '·à§', 36: '·à•', 37: '·à¶', 38: '·àß',
    39: '·à®', 40: '·à©', 41: '·à™', 42: '·à´', 43: '·à¨', 44: '·à≠', 45: '·àÆ', 46: '·àØ',
    47: '·à∞', 48: '·à±', 49: '·à≤', 50: '·à≥', 51: '·à¥', 52: '·àµ', 53: '·à∂', 54: '·à∑',
    55: '·à∏', 56: '·àπ', 57: '·à∫', 58: '·àª', 59: '·àº', 60: '·àΩ', 61: '·àæ', 62: '·àø',
    63: '·âÄ', 64: '·âÅ', 65: '·âÇ', 66: '·âÉ', 67: '·âÑ', 68: '·âÖ', 69: '·âÜ', 70: '·âã',
    71: '·â†', 72: '·â°', 73: '·â¢', 74: '·â£', 75: '·â§', 76: '·â•', 77: '·â¶', 78: '·âß',
    79: '·â®', 80: '·â©', 81: '·â™', 82: '·â´', 83: '·â¨', 84: '·â≠', 85: '·âÆ', 86: '·âØ',
    87: '·â∞', 88: '·â±', 89: '·â≤', 90: '·â≥', 91: '·â¥', 92: '·âµ', 93: '·â∂', 94: '·â∑',
    95: '·â∏', 96: '·âπ', 97: '·â∫', 98: '·âª', 99: '·âº', 100: '·âΩ', 101: '·âæ', 102: '·âø',
    103: '·äÄ', 104: '·äÅ', 105: '·äÇ', 106: '·äÉ', 107: '·äÑ', 108: '·äÖ', 109: '·äÜ',
    110: '·äê', 111: '·äë', 112: '·äí', 113: '·äì', 114: '·äî', 115: '·äï', 116: '·äñ', 117: '·äó',
    118: '·äò', 119: '·äô', 120: '·äö', 121: '·äõ', 122: '·äú', 123: '·äù', 124: '·äû', 125: '·äü',
    126: '·ä†', 127: '·ä°', 128: '·ä¢', 129: '·ä£', 130: '·ä§', 131: '·ä•', 132: '·ä¶', 133: '·äß',
    134: '·ä®', 135: '·ä©', 136: '·ä™', 137: '·ä´', 138: '·ä¨', 139: '·ä≠', 140: '·äÆ', 141: '·äØ',
    142: '·ä∏', 143: '·äπ', 144: '·ä∫', 145: '·äª', 146: '·äº', 147: '·äΩ', 148: '·äæ', 149: '·ãÄ', 150: '·ãÉ',
    151: '·ãà', 152: '·ãâ', 153: '·ãä', 154: '·ãã', 155: '·ãå', 156: '·ãç', 157: '·ãé', 158: '·ãè',
    159: '·ãê', 160: '·ãë', 161: '·ãí', 162: '·ãì', 163: '·ãî', 164: '·ãï', 165: '·ãñ',
    166: '·ãò', 167: '·ãô', 168: '·ãö', 169: '·ãõ', 170: '·ãú', 171: '·ãù', 172: '·ãû', 173: '·ãü',
    174: '·ã†', 175: '·ã°', 176: '·ã¢', 177: '·ã£', 178: '·ã§', 179: '·ã•', 180: '·ã¶', 181: '·ãß',
    182: '·ã®', 183: '·ã©', 184: '·ã™', 185: '·ã´', 186: '·ã¨', 187: '·ã≠', 188: '·ãÆ',
    189: '·ã∞', 190: '·ã±', 191: '·ã≤', 192: '·ã≥', 193: '·ã¥', 194: '·ãµ', 195: '·ã∂', 196: '·ã∑',
    197: '·åÄ', 198: '·åÅ', 199: '·åÇ', 200: '·åÉ', 201: '·åÑ', 202: '·åÖ', 203: '·åÜ', 204: '·åá',
    205: '·åà', 206: '·åâ', 207: '·åä', 208: '·åã', 209: '·åå', 210: '·åç', 211: '·åé', 212: '·åè',
    213: '·å†', 214: '·å°', 215: '·å¢', 216: '·å£', 217: '·å§', 218: '·å•', 219: '·å¶', 220: '·åß',
    221: '·å®', 222: '·å©', 223: '·å™', 224: '·å´', 225: '·å¨', 226: '·å≠', 227: '·åÆ', 228: '·åØ',
    229: '·å∞', 230: '·å±', 231: '·å≤', 232: '·å≥', 233: '·å¥', 234: '·åµ', 235: '·å∂', 236: '·å∑',
    237: '·å∏', 238: '·åπ', 239: '·å∫', 240: '·åª', 241: '·åº', 242: '·åΩ', 243: '·åæ', 244: '·åø',
    245: '·çÄ', 246: '·çÅ', 247: '·çÇ', 248: '·çÉ', 249: '·çÑ', 250: '·çÖ', 251: '·çÜ', 252: '·çá',
    253: '·çà', 254: '·çâ', 255: '·çä', 256: '·çã', 257: '·çå', 258: '·çç', 259: '·çé', 260: '·çè',
    261: '·çê', 262: '·çë', 263: '·çí', 264: '·çì', 265: '·çî', 266: '·çï', 267: '·çñ', 268: '·çó',

    # Special characters / punctuation / numbers
    269: "!", 270: ":-", 271: "<", 272: "(", 273: "¬´", 274: "·ç•", 275: "%", 276: "¬ª", 277: ")",
    278: ">", 279: ".", 280: "+", 281: "·ç£", 282: "-", 283: "·ç¢", 284: "/", 
    285: "0", 286: "1", 287: "2", 288: "3", 289: "4", 290: "5", 291: "6", 292: "7", 293: "8", 294: "9", 
    295: "·ç°", 296: "·ç§", 297: "...", 298: "*", 299: "#", 300: "?"
}

import matplotlib.pyplot as plt
def CNNR_Interface(img_path):
    img = preprocess_image(img_path)
    ig = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    plt.imshow(ig)
    plt.axis('off') # Turn off axis labels and ticks
    plt.title(img_path)
    plt.show()
    with torch.no_grad():
        output = model(img)  # [B, T, nclass]
    
    predicted_text = ctc_greedy_decoder(output, amharic_mapping, blank=BLANK)
    print("Predicted:", predicted_text)
    return predicted_text

CNNR_Interface("/kaggle/working/cropped_words/word_1.jpg")

import os
import shutil

def clear_folder(folder_path):
    """
    Ensures the folder exists, then deletes all files and subdirectories inside it.
    
    Args:
        folder_path (str): Path to the folder to clear.
    """
    # Create folder if it does not exist
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"üìÅ Created folder '{folder_path}'")
        return

    # Clear existing contents
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.remove(file_path)  # Remove file or symlink
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)  # Remove subdirectory
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to delete {file_path}. Reason: {e}")
    
    print(f"‚úÖ Cleared all contents in '{folder_path}'")

def pipeline(img_path):
    detected_text = ''
    clear_folder('/kaggle/working/cropped_words')
    print('done cleaning')
    YOLO_Interface(img_path)
    print('done locating')
    locs = YOLO_cropper(img_path)
    print('done cropping')
    
    if len(locs) == 0:
        return detected_text
    for loc in locs:
        print(loc)
        detected_text += CNNR_Interface(loc)
    
    return detected_text

import os
import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from ultralytics import YOLO
print(pipeline('/kaggle/input/dataseet/data/images/test/aahw_0007.jpg'))
 
